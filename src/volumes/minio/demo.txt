#Setup the Minio bucket which we sink data to
docker exec -ti mc sh -c "mc config host add minio http://minio:9000 username password"
docker exec -ti mc sh -c "mc mb minio/kafka"

# Create the topic
docker exec -ti kafka1 kafka-topics --bootstrap-server kafka1:9092 --create --topic users --replication-factor 3 --partitions 2

# Generate records
docker exec -ti connect sh -c "confluent-hub install --no-prompt confluentinc/kafka-connect-datagen:latest"
docker restart connect
docker exec -ti connect /bin/bash -c 'curl -vX POST http://connect:8083/connectors --header "Content-Type: application/json" --data @/volumes/datagen_users.json'

# Shutdown record generator
docker exec -ti connect curl -vX DELETE http://connect:8083/connectors/datagen-users

# Consume the records from each partition
docker exec -ti schemaregistry kafka-avro-console-consumer --bootstrap-server kafka1:9092 --topic users --property schema.registry.url=http://schemaregistry:8081 --partition 0 --from-beginning |grep registertime >> partition_0.log
docker exec -ti schemaregistry kafka-avro-console-consumer --bootstrap-server kafka1:9092 --topic users --property schema.registry.url=http://schemaregistry:8081 --partition 1 --from-beginning |grep registertime >> partition_1.log

# Add and deploy the sink connector
docker exec -ti connect sh -c "confluent-hub install --no-prompt confluentinc/kafka-connect-s3:latest"
docker restart connect
docker exec -ti minio sh -c "bash /volumes/deploy-sink-connector.sh"

# Cleanup the sink connector and topic
docker exec -ti connect curl -vX DELETE http://connect:8083/connectors/s3-backup
docker exec -ti kafka1 kafka-topics --bootstrap-server kafka1:9092 --delete --topic users

# Recreate the topic
docker exec -ti kafka1 kafka-topics --bootstrap-server kafka1:9092 --create --topic users --replication-factor 3 --partitions 2

# Add and create the source connector
docker exec -ti connect sh -c "confluent-hub install --no-prompt confluentinc/kafka-connect-s3-source:latest"
docker restart connect
docker exec -ti minio sh -c "bash /volumes/deploy-source-connector.sh"

# Consume the records and validate they match, with the correct partition and maintaining record ordering
docker exec -ti schemaregistry kafka-avro-console-consumer --bootstrap-server kafka1:9092 --topic users --property schema.registry.url=http://schemaregistry:8081 --partition 0 --from-beginning |grep registertime >> partition_0.log
docker exec -ti schemaregistry kafka-avro-console-consumer --bootstrap-server kafka1:9092 --topic users --property schema.registry.url=http://schemaregistry:8081 --partition 1 --from-beginning |grep registertime >> partition_1.log
